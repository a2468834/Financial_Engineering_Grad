# -*- coding: utf-8 -*-
"""Fintech NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mT9-xwqty6LXf_P4gTuKiDeQHLY6CAq0
"""

# Commented out IPython magic to ensure Python compatibility.
# inline plotting instead of popping out
# %matplotlib inline
import re
import os
import numpy as np # numpy  1.17.1
import pandas as pd # pandas  0.25.1
import matplotlib.pyplot as plt # matplotlib 3.1.1
import seaborn as sns # seaborn 0.9.0
from sklearn.preprocessing import StandardScaler # scikit-learn 0.21.3
import scipy.stats



# inline plotting instead of popping out
# %matplotlib inline

# python 3.7.3
import os, itertools, csv

from IPython.display import Image
from IPython.display import display

# numpy  1.17.1
import numpy as np

# pandas  0.25.1
import pandas as pd

# scikit-learn  0.21.3
from sklearn.compose import ColumnTransformer
from sklearn.datasets import make_moons
from sklearn.impute import SimpleImputer 
from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.svm import SVC

# matplotlib  3.1.1
import matplotlib
matplotlib.rcParams.update({'font.size': 22})
plt = matplotlib.pyplot

from google.colab import drive
import pandas as pd
drive.mount('/content/gdrive') # 此處需要登入google帳號
# 獲取授權碼之後輸入即可連動雲端硬碟
X_train = np.load('/content/gdrive/My Drive/fintech/proper X_train.npy')
X_test = np.load('/content/gdrive/My Drive/fintech/proper X_test.npy')
Y_train = np.load('/content/gdrive/My Drive/fintech/proper Y_train.npy')
Y_test = pd.read_csv("/content/gdrive/My Drive/fintech/Y_test.csv")

#################################################  少塞N Y正常全放 的code   0.98/0.98
# normalization
mean = np.mean(X_train[:,-109:],axis=0)#對直線取mean
#std = np.std(X_train[:,-109:],axis=0)
max = np.max(X_train[:,-109:],axis=0)

#X_train[:,-109:] -= mean
X_train[:,-109:] /= max
#X_test[:,-109:] -= mean
X_test[:,-109:] /= max

X_train = X_train.astype("float32")


Nindex = np.where(Y_train==0)
Yindex = np.where(Y_train==1)
Nindex = list(Nindex[0])
Yindex = list(Yindex[0])


from random import sample 

sample_index = sample(Nindex,298637)
sample_index.extend(Yindex) #接下來可直接塞入train

import random
random.shuffle(sample_index) #重排一下打散樣本


sample_X_train = X_train[sample_index,:]
sample_Y_train = Y_train[sample_index]

print('ok')

## normalization
#mean = np.mean(X_train[:,-109:],axis=0)#對直線取mean
##std = np.std(X_train[:,-109:],axis=0)
#max = np.max(X_train[:,-109:],axis=0)

##X_train[:,-109:] -= mean
#X_train[:,-109:] /= max
##X_test[:,-109:] -= mean
#X_test[:,-109:] /= max

#X_train = X_train.astype("float32")

#X_train,X_val,Y_train,Y_val = train_test_split(X_train,Y_train,test_size=0.2, random_state=0)

############################################# 嘗試多塞Y 不動N####################  val f1 0.9515
#Nindex = np.where(Y_train==0)
#Yindex = np.where(Y_train==1)
#Nindex = list(Nindex[0])    #1467496
#Yindex = list(Yindex[0])    #238717


#from random import sample 

#sample_index = Nindex #sample(Nindex,298637)
#sample_index.extend(Yindex) #接下來可直接塞入train
#sample_index.extend(Yindex)
#sample_index.extend(Yindex)
#sample_index.extend(Yindex)
#sample_index.extend(Yindex)
#sample_index.extend(Yindex)

#import random
#random.shuffle(sample_index) #重排一下打散樣本


#sample_X_train = X_train[sample_index,:]
#sample_Y_train = Y_train[sample_index]

#print('ok')

# 1467496/238717

#from keras.utils import np_utils
#Y_train=np_utils.to_categorical(Y_train)

#len(Yindex)# 298637個Y
#len(Nindex)# 1834130個N

#sample_X_train.shape   # 597274,253

import tensorflow as tf 
#import keras
#from keras import models,layers
#import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
#inputs = keras.Input(shape=(253,))
##x = layers.Dense(128)(inputs)
#x = layers.Dense(128,activation='relu')(inputs)
#x = layers.Dropout(0.3)(x)
##x = layers.Dense(128)(x)
#x = layers.Dense(128,activation='relu')(x)
#x = layers.Dropout(0.3)(x)
##x = layers.Dense(64)(x)
#x = layers.Dense(64,activation='relu')(x)
#x = layers.Dropout(0.3)(x)
##x = layers.Dense(16)(x)
#x = layers.Dense(16,activation='relu')(inputs)
#x = layers.Dropout(0.3)(x)
#outputs = layers.Dense(1)(x)
#outputs = layers.Dense(1,activation='sigmoid')(x)
#model = keras.Model(inputs,outputs,name='model')
#model.summary()

import tensorflow as tf 
from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(253,))
x = layers.Dense(512,activation='relu')(inputs) #64
x = layers.Dropout(0.3)(x)
x = layers.Dense(512,activation='relu')(x)  #64
x = layers.Dropout(0.3)(x)
x = layers.Dense(256,activation='relu')(x)  #16
x = layers.Dropout(0.3)(x)
x = layers.Dense(128,activation='relu')(x)  #
#x = layers.Dropout(0.3)(x)
x = layers.Dense(64,activation='relu')(x)
x = layers.Dense(32,activation='relu')(x)  #
#x = layers.Dropout(0.3)(x)
outputs = layers.Dense(1,activation='sigmoid')(x)


model = keras.Model(inputs,outputs,name='model')
#model.summary()
print('ok')

from keras.losses import binary_crossentropy, categorical_crossentropy
import keras.backend as K
import numpy as np
from prettytable import PrettyTable
from prettytable import ALL
from sklearn.metrics import f1_score
from matplotlib import pyplot as plt

def f1(y_true, y_pred):
    y_pred = K.round(y_pred)
    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

    p = tp / (tp + fp + K.epsilon())
    r = tp / (tp + fn + K.epsilon())

    f1 = 2*p*r / (p+r+K.epsilon())
    #f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
    return K.mean(f1)

def f1_loss(y_true, y_pred):
    
    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)
    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)
    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)
    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)

    p = tp / (tp + fp + K.epsilon())
    r = tp / (tp + fn + K.epsilon())

    f1 = 2*p*r / (p+r+K.epsilon())
#    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)
    return 1 - K.mean(f1)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

#(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

#x_train = x_train.reshape(60000, 784).astype("float32") / 255
#x_test = x_test.reshape(10000, 784).astype("float32") / 255

#model.compile(loss=keras.losses.BinaryCrossentropy(),optimizer=keras.optimizers.Adam(),metrics=["accuracy"])

#model.compile(loss=keras.losses.BinaryCrossentropy(),optimizer=keras.optimizers.Adam(),metrics=["accuracy",f1])

#model.compile(optimizer='adam', loss=f1_loss, metrics=['accuracy', f1])

history = model.fit(sample_X_train, sample_Y_train, batch_size=2048, epochs=40, validation_split=0.2)
#history = model.fit(sample_X_train, sample_Y_train, batch_size=2048, epochs=80, validation_data=(X_val,Y_val))


#test_scores = model.evaluate(x_test, y_test, verbose=2)
#print("Test loss:", test_scores[0])
#print("Test accuracy:", test_scores[1])

history = model.fit(X_train, Y_train, batch_size=2048, epochs=10)

#history = model.fit(X_train, Y_train, batch_size=2048, epochs=40,validation_split=0.2)

###  cutpoint的處理?  似乎0.5還是比較好
## 可以每10個epochs 換一組sampleX_train 重抽 Y依樣 N重抽 依樣分0.2validation
#下一大步:漸進式learning rate
#  更改loss function
#  更改data 對NA的處理

prediction = model.predict(X_train)

prediction = np.where(prediction>0.5,1,0)

ps = pd.Series([tuple(i) for i in zip(Y_train,prediction)])
counts = ps.value_counts()
print(counts)

#0 是 negative
TN = 1832427   #  true0   pred0
TP = 284929   # 1 1
FP = 13708    # 1 0
FN = 1703     # 0 1

Recall = TP/(TP+FN)
Precision = TP/(TP+FP)
F1 = 2 * Precision * Recall / (Precision + Recall)

print(F1)
# 0.9736685182369134   0.5 cutpoint

TN = 1832427   #  true0   pred0
TP = 284929   # 1 1
FP = 13708    # 1 0
FN = 1703     # 0 1
print('acc:',(TN+TP)/(TN+TP+FP+FN)) #0.99277417551904074



prediction_for_Y_test = model.predict(X_test)

prediction_for_Y_test = np.where(prediction_for_Y_test>0.5,'Y','N')

prediction_for_Y_test = prediction_for_Y_test[:,0]

Y_test['loan_status'] = prediction_for_Y_test

Y_test

Y_test.to_csv('/content/gdrive/My Drive/fintech/12_1try1.csv',index=0)
print('ok')

# model.save('/content/gdrive/My Drive/fintech/12_01_try1.h5') 最好model

model = keras.models.load_model('/content/gdrive/My Drive/fintech/12_01_try1.h5')

(Y_test['loan_status']=='Y').sum()

Y_test = pd.read_csv("/content/gdrive/My Drive/fintech/12_1try1.csv")

#0 是 negative
TN = 366548   #  true0   pred0
TP = 55612   # 1 1
FP = 4308    # 1 0
FN = 86     # 0 1

Recall = TP/(TP+FN)
Precision = TP/(TP+FP)
F1 = 2 * Precision * Recall / (Precision + Recall)

print(F1)













#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

prediction = model.predict(testdata)

